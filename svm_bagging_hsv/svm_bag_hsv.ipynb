{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ensemble Bagging SVM with Optuna Hyperparameter Tuning\n",
    "Feature Extraction: HSV Histogram ONLY\n",
    "Dataset: 54 Indonesian Coffee Bean Varieties\n",
    "Platform: Kaggle\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, confusion_matrix, \n",
    "    classification_report, top_k_accuracy_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import pickle\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d6d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CONFIGURATION \n",
    "class Config:\n",
    "    # Data paths\n",
    "    DATA_DIR = '/kaggle/input/coffee-bean-dataset'\n",
    "    TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "    VAL_DIR = os.path.join(DATA_DIR, 'valid')\n",
    "    TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "    \n",
    "    # Model configuration\n",
    "    MODEL_NAME = 'Bagging_SVM_HSV_Only'\n",
    "    NUM_CLASSES = 54\n",
    "    IMG_SIZE = 224\n",
    "    \n",
    "    # Feature extraction configuration\n",
    "    # HSV Histogram bins\n",
    "    HSV_BINS = (8, 8, 8)  # H, S, V bins\n",
    "    \n",
    "    # Optuna configuration\n",
    "    N_TRIALS = 50  # Number of Optuna trials\n",
    "    OPTUNA_TIMEOUT = 3600  # 1 hour timeout\n",
    "    \n",
    "    # Output\n",
    "    SAVE_DIR = './output_svm_hsv'\n",
    "    MODEL_PATH = os.path.join(SAVE_DIR, 'best_bagging_svm_hsv_model.pkl')\n",
    "    SCALER_PATH = os.path.join(SAVE_DIR, 'scaler.pkl')\n",
    "    LABEL_ENCODER_PATH = os.path.join(SAVE_DIR, 'label_encoder.pkl')\n",
    "# Create output directory\n",
    "os.makedirs(Config.SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  FEATURE EXTRACTION \n",
    "class FeatureExtractor:\n",
    "    \"\"\"Extract HSV features ONLY from coffee bean images\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, hsv_bins=(8, 8, 8)):\n",
    "        self.img_size = img_size\n",
    "        self.hsv_bins = hsv_bins\n",
    "    \n",
    "    def extract_hsv_histogram(self, image):\n",
    "        \"\"\"Extract HSV color histogram features\"\"\"\n",
    "        # Convert to HSV\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # Calculate histogram for each channel\n",
    "        hist_h = cv2.calcHist([hsv], [0], None, [self.hsv_bins[0]], [0, 180])\n",
    "        hist_s = cv2.calcHist([hsv], [1], None, [self.hsv_bins[1]], [0, 256])\n",
    "        hist_v = cv2.calcHist([hsv], [2], None, [self.hsv_bins[2]], [0, 256])\n",
    "        \n",
    "        # Normalize histograms\n",
    "        hist_h = cv2.normalize(hist_h, hist_h).flatten()\n",
    "        hist_s = cv2.normalize(hist_s, hist_s).flatten()\n",
    "        hist_v = cv2.normalize(hist_v, hist_v).flatten()\n",
    "        \n",
    "        # Concatenate all histograms\n",
    "        hsv_features = np.concatenate([hist_h, hist_s, hist_v])\n",
    "        \n",
    "        return hsv_features\n",
    "    \n",
    "    def extract_features(self, image_path):\n",
    "        \"\"\"Extract HSV features from an image\"\"\"\n",
    "        # Load and resize image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (self.img_size, self.img_size))\n",
    "        \n",
    "        # Extract HSV features ONLY\n",
    "        hsv_features = self.extract_hsv_histogram(image)\n",
    "        \n",
    "        return hsv_features\n",
    "    \n",
    "    def extract_from_directory(self, directory):\n",
    "        \"\"\"Extract features from all images in directory\"\"\"\n",
    "        features = []\n",
    "        labels = []\n",
    "        class_names = sorted(os.listdir(directory))\n",
    "        \n",
    "        print(f\"\\nExtracting features from {directory}...\")\n",
    "        for class_name in tqdm(class_names, desc='Classes'):\n",
    "            class_dir = os.path.join(directory, class_name)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "            \n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    try:\n",
    "                        feature = self.extract_features(img_path)\n",
    "                        features.append(feature)\n",
    "                        labels.append(class_name)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {img_path}: {e}\")\n",
    "        \n",
    "        return np.array(features), np.array(labels), class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7095944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  OPTUNA OPTIMIZATION \n",
    "class OptunaOptimizer:\n",
    "    \"\"\"Optimize Bagging SVM hyperparameters using Optuna\"\"\"\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.best_params = None\n",
    "        self.best_score = 0.0\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \"\"\"Optuna objective function\"\"\"\n",
    "        # SVM hyperparameters\n",
    "        C = trial.suggest_float('C', 0.1, 100.0, log=True)\n",
    "        kernel = trial.suggest_categorical('kernel', ['rbf', 'poly', 'sigmoid'])\n",
    "        \n",
    "        if kernel == 'rbf':\n",
    "            gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "        elif kernel == 'poly':\n",
    "            gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "            degree = trial.suggest_int('degree', 2, 5)\n",
    "        else:  # sigmoid\n",
    "            gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "        \n",
    "        # Bagging hyperparameters\n",
    "        n_estimators = trial.suggest_int('n_estimators', 5, 50)\n",
    "        max_samples = trial.suggest_float('max_samples', 0.5, 1.0)\n",
    "        max_features = trial.suggest_float('max_features', 0.5, 1.0)\n",
    "        \n",
    "        # Create base SVM\n",
    "        svm_params = {\n",
    "            'C': C,\n",
    "            'kernel': kernel,\n",
    "            'gamma': gamma,\n",
    "            'probability': True,\n",
    "            'random_state': 42,\n",
    "            'cache_size': 1000\n",
    "        }\n",
    "        \n",
    "        if kernel == 'poly':\n",
    "            svm_params['degree'] = degree\n",
    "        \n",
    "        base_svm = SVC(**svm_params)\n",
    "        \n",
    "        # Create Bagging ensemble\n",
    "        bagging = BaggingClassifier(\n",
    "            estimator=base_svm,\n",
    "            n_estimators=n_estimators,\n",
    "            max_samples=max_samples,\n",
    "            max_features=max_features,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Train on training set\n",
    "        bagging.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        y_pred = bagging.predict(self.X_val)\n",
    "        accuracy = accuracy_score(self.y_val, y_pred)\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def optimize(self, n_trials=50, timeout=3600):\n",
    "        \"\"\"Run Optuna optimization\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Starting Optuna Hyperparameter Optimization\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=TPESampler(seed=42)\n",
    "        )\n",
    "        \n",
    "        study.optimize(\n",
    "            self.objective,\n",
    "            n_trials=n_trials,\n",
    "            timeout=timeout,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        self.best_params = study.best_params\n",
    "        self.best_score = study.best_value\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Optimization Completed!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Best Validation Accuracy: {self.best_score:.4f}\")\n",
    "        print(f\"Best Parameters:\")\n",
    "        for key, value in self.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return self.best_params, study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61b0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  MODEL TRAINING \n",
    "def train_best_model(X_train, y_train, best_params):\n",
    "    \"\"\"Train final model with best parameters\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training Final Model with Best Parameters\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Extract parameters\n",
    "    C = best_params['C']\n",
    "    kernel = best_params['kernel']\n",
    "    gamma = best_params['gamma']\n",
    "    n_estimators = best_params['n_estimators']\n",
    "    max_samples = best_params['max_samples']\n",
    "    max_features = best_params['max_features']\n",
    "    \n",
    "    # Create base SVM\n",
    "    svm_params = {\n",
    "        'C': C,\n",
    "        'kernel': kernel,\n",
    "        'gamma': gamma,\n",
    "        'probability': True,\n",
    "        'random_state': 42,\n",
    "        'cache_size': 1000\n",
    "    }\n",
    "    \n",
    "    if kernel == 'poly' and 'degree' in best_params:\n",
    "        svm_params['degree'] = best_params['degree']\n",
    "    \n",
    "    base_svm = SVC(**svm_params)\n",
    "    \n",
    "    # Create Bagging ensemble\n",
    "    model = BaggingClassifier(\n",
    "        estimator=base_svm,\n",
    "        n_estimators=n_estimators,\n",
    "        max_samples=max_samples,\n",
    "        max_features=max_features,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nTraining model...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7721d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  EVALUATION \n",
    "def evaluate_model(model, X_test, y_test, label_encoder, class_names):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Evaluating Model on Test Set\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Predictions\n",
    "    print(\"Making predictions...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    top1_acc = 100. * accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Top-5 Accuracy\n",
    "    top5_acc = 100. * top_k_accuracy_score(\n",
    "        y_test, y_proba, k=5, \n",
    "        labels=range(len(label_encoder.classes_))\n",
    "    )\n",
    "    \n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEST RESULTS - BAGGING SVM (HSV ONLY):\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Top-1 Accuracy: {top1_acc:.2f}%\")\n",
    "    print(f\"Top-5 Accuracy: {top5_acc:.2f}%\")\n",
    "    print(f\"Macro F1-Score: {macro_f1:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    results = {\n",
    "        'predictions': y_pred,\n",
    "        'labels': y_test,\n",
    "        'probabilities': y_proba,\n",
    "        'top1_accuracy': top1_acc,\n",
    "        'top5_accuracy': top5_acc,\n",
    "        'macro_f1': macro_f1\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a4f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  VISUALIZATION \n",
    "def plot_confusion_matrix(y_test, y_pred, class_names):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    # Ensure we only use classes that exist in the test set\n",
    "    unique_labels = np.unique(y_test)\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=unique_labels)\n",
    "    actual_class_names = [class_names[i] for i in unique_labels]\n",
    "    \n",
    "    plt.figure(figsize=(20, 18))\n",
    "    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues',\n",
    "                xticklabels=actual_class_names, yticklabels=actual_class_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.title('Confusion Matrix - Bagging SVM (HSV Only)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=90, fontsize=8)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.SAVE_DIR, 'confusion_matrix_svm_hsv.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "def analyze_per_class_performance(y_test, y_pred, class_names):\n",
    "    \"\"\"Analyze per-class performance\"\"\"\n",
    "    # Ensure we only use classes that exist in the test set\n",
    "    unique_labels = np.unique(y_test)\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=unique_labels)\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    \n",
    "    # Get corresponding class names\n",
    "    actual_class_names = [class_names[i] for i in unique_labels]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    class_analysis = pd.DataFrame({\n",
    "        'Class': actual_class_names,\n",
    "        'Accuracy': per_class_acc * 100,\n",
    "        'Samples': cm.sum(axis=1)\n",
    "    })\n",
    "    class_analysis = class_analysis.sort_values('Accuracy')\n",
    "    \n",
    "    print(\"\\nPer-Class Performance:\")\n",
    "    print(\"=\"*60)\n",
    "    print(class_analysis.to_string(index=False))\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Top 5 best and worst\n",
    "    print(\"\\nðŸ“Š TOP 5 BEST PERFORMING CLASSES:\")\n",
    "    print(class_analysis.tail(5).to_string(index=False))\n",
    "    \n",
    "    print(\"\\nðŸ“‰ TOP 5 WORST PERFORMING CLASSES:\")\n",
    "    print(class_analysis.head(5).to_string(index=False))\n",
    "    \n",
    "    # Save to CSV\n",
    "    class_analysis.to_csv(\n",
    "        os.path.join(Config.SAVE_DIR, 'per_class_analysis_svm_hsv.csv'), \n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    return class_analysis\n",
    "def plot_optuna_study(study):\n",
    "    \"\"\"Plot Optuna optimization history\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Optimization history\n",
    "    trials = study.trials\n",
    "    values = [t.value for t in trials]\n",
    "    \n",
    "    axes[0].plot(values, marker='o', linestyle='-', alpha=0.7)\n",
    "    axes[0].axhline(y=study.best_value, color='r', \n",
    "                    linestyle='--', label=f'Best: {study.best_value:.4f}')\n",
    "    axes[0].set_xlabel('Trial')\n",
    "    axes[0].set_ylabel('Validation Accuracy')\n",
    "    axes[0].set_title('Optuna Optimization History')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Parameter importance (if available)\n",
    "    try:\n",
    "        importance = optuna.importance.get_param_importances(study)\n",
    "        params = list(importance.keys())\n",
    "        importances = list(importance.values())\n",
    "        \n",
    "        axes[1].barh(params, importances)\n",
    "        axes[1].set_xlabel('Importance')\n",
    "        axes[1].set_title('Hyperparameter Importance')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    except Exception as e:\n",
    "        axes[1].text(0.5, 0.5, 'Parameter importance\\nnot available', \n",
    "                    ha='center', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.SAVE_DIR, 'optuna_study_hsv.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "#  MAIN EXECUTION \n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"Bagging SVM with Optuna - Coffee Bean Classification\")\n",
    "    print(\"Feature Extraction: HSV ONLY\")\n",
    "    print(\"54 Indonesian Coffee Varieties\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize feature extractor\n",
    "    feature_extractor = FeatureExtractor(\n",
    "        img_size=Config.IMG_SIZE,\n",
    "        hsv_bins=Config.HSV_BINS\n",
    "    )\n",
    "    \n",
    "    # Extract features from all datasets\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Feature Extraction Phase\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    X_train, y_train, class_names = feature_extractor.extract_from_directory(Config.TRAIN_DIR)\n",
    "    X_val, y_val, _ = feature_extractor.extract_from_directory(Config.VAL_DIR)\n",
    "    X_test, y_test, _ = feature_extractor.extract_from_directory(Config.TEST_DIR)\n",
    "    \n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Train samples: {len(X_train)}\")\n",
    "    print(f\"  Validation samples: {len(X_val)}\")\n",
    "    print(f\"  Test samples: {len(X_test)}\")\n",
    "    print(f\"  Number of classes: {len(class_names)}\")\n",
    "    print(f\"  Feature dimension: {X_train.shape[1]}\")\n",
    "    print(f\"    - HSV features: {sum(Config.HSV_BINS)}\")\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_val_encoded = label_encoder.transform(y_val)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "    \n",
    "    # Feature scaling\n",
    "    print(\"\\nScaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Save scaler and label encoder\n",
    "    joblib.dump(scaler, Config.SCALER_PATH)\n",
    "    joblib.dump(label_encoder, Config.LABEL_ENCODER_PATH)\n",
    "    print(f\"Scaler saved to {Config.SCALER_PATH}\")\n",
    "    print(f\"Label encoder saved to {Config.LABEL_ENCODER_PATH}\")\n",
    "    \n",
    "    # Optuna optimization\n",
    "    optimizer = OptunaOptimizer(\n",
    "        X_train_scaled, y_train_encoded,\n",
    "        X_val_scaled, y_val_encoded\n",
    "    )\n",
    "    \n",
    "    best_params, study = optimizer.optimize(\n",
    "        n_trials=Config.N_TRIALS,\n",
    "        timeout=Config.OPTUNA_TIMEOUT\n",
    "    )\n",
    "    \n",
    "    # Plot Optuna study\n",
    "    plot_optuna_study(study)\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    model = train_best_model(X_train_scaled, y_train_encoded, best_params)\n",
    "    \n",
    "    # Save model\n",
    "    print(f\"\\nSaving model to {Config.MODEL_PATH}...\")\n",
    "    joblib.dump(model, Config.MODEL_PATH)\n",
    "    print(\"Model saved successfully!\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    results = evaluate_model(\n",
    "        model, X_test_scaled, y_test_encoded,\n",
    "        label_encoder, class_names\n",
    "    )\n",
    "    \n",
    "    # Visualizations\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    plot_confusion_matrix(y_test_encoded, results['predictions'], class_names)\n",
    "    analyze_per_class_performance(y_test_encoded, results['predictions'], class_names)\n",
    "    \n",
    "    # Save final results\n",
    "    final_results = {\n",
    "        'model': Config.MODEL_NAME,\n",
    "        'num_classes': Config.NUM_CLASSES,\n",
    "        'feature_dim': X_train.shape[1],\n",
    "        'hsv_bins': Config.HSV_BINS,\n",
    "        'best_params': best_params,\n",
    "        'top1_accuracy': results['top1_accuracy'],\n",
    "        'top5_accuracy': results['top5_accuracy'],\n",
    "        'macro_f1': results['macro_f1'],\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val),\n",
    "        'test_samples': len(X_test)\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(os.path.join(Config.SAVE_DIR, 'final_results_svm_hsv.json'), 'w') as f:\n",
    "        json.dump(final_results, f, indent=4)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… Training and evaluation completed successfully!\")\n",
    "    print(f\"Results saved to: {Config.SAVE_DIR}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
