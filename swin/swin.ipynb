{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e62b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Swin Transformer V1 Tiny for Coffee Bean Classification\n",
    "Dataset: 54 Indonesian Coffee Bean Varieties\n",
    "Platform: Kaggle with GPU T4 x2\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, top_k_accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b6a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e20331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "class Config:\n",
    "    # Data paths \n",
    "    DATA_DIR = '/kaggle/input/dataset' \n",
    "    TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "    VAL_DIR = os.path.join(DATA_DIR, 'valid')\n",
    "    TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "    \n",
    "    # Model configuration\n",
    "    MODEL_NAME = 'swin_tiny_patch4_window7_224'\n",
    "    NUM_CLASSES = 54\n",
    "    IMG_SIZE = 224\n",
    "    \n",
    "    # Training configuration\n",
    "    BATCH_SIZE = 32  # Adjust based on GPU memory\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 0.05\n",
    "    NUM_WORKERS = 4\n",
    "    \n",
    "    # Device configuration - Use both GPUs\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    USE_MULTI_GPU = torch.cuda.device_count() > 1\n",
    "    \n",
    "    # Training settings\n",
    "    EARLY_STOPPING_PATIENCE = 10\n",
    "    SAVE_DIR = './output'\n",
    "    CHECKPOINT_PATH = os.path.join(SAVE_DIR, 'best_swin_model.pth')\n",
    "    \n",
    "    # Fine-tuning settings\n",
    "    FINETUNE_LAYERS = 1.0  # 1.0 = fine-tune all layers, 0.5 = fine-tune 50% of layers\n",
    "# Create output directory\n",
    "os.makedirs(Config.SAVE_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fab35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET\n",
    "class CoffeeDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for Coffee Bean Classification\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load all image paths and labels\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        self.images.append(os.path.join(class_dir, img_name))\n",
    "                        self.labels.append(self.class_to_idx[class_name])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459a8409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA AUGMENTATION \n",
    "def get_transforms(split='train'):\n",
    "    \"\"\"Get appropriate transforms for each data split\"\"\"\n",
    "    \n",
    "    if split == 'train':\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=10),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:  # validation and test\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf4e7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "def create_model(num_classes=54, pretrained=True):\n",
    "    \"\"\"Create Swin Transformer V1 Tiny model\"\"\"\n",
    "    \n",
    "    print(f\"Creating {Config.MODEL_NAME} model...\")\n",
    "    model = timm.create_model(\n",
    "        Config.MODEL_NAME,\n",
    "        pretrained=pretrained,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    \n",
    "    # Print model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47fe9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, criterion, optimizer, scheduler, device):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        \n",
    "        self.best_val_acc = 0.0\n",
    "        self.best_val_f1 = 0.0\n",
    "        self.epochs_without_improvement = 0\n",
    "        self.history = {\n",
    "            'train_loss': [], 'train_acc': [],\n",
    "            'val_loss': [], 'val_acc': [], 'val_f1': []\n",
    "        }\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc='Training')\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = 100. * correct / total\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(self.val_loader, desc='Validation')\n",
    "            for images, labels in pbar:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                # Get probabilities for top-k accuracy\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                \n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                all_probs.append(probs.cpu().numpy())\n",
    "                all_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        val_loss = running_loss / len(all_labels)\n",
    "        val_acc = 100. * accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        # Calculate Top-5 Accuracy\n",
    "        all_probs_concat = np.vstack(all_probs)\n",
    "        top5_acc = 100. * top_k_accuracy_score(\n",
    "            all_labels, all_probs_concat, k=5, labels=range(Config.NUM_CLASSES)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nValidation - Loss: {val_loss:.4f}, Top-1 Acc: {val_acc:.2f}%, \"\n",
    "              f\"Top-5 Acc: {top5_acc:.2f}%, Macro F1: {val_f1:.4f}\")\n",
    "        \n",
    "        return val_loss, val_acc, val_f1, top5_acc\n",
    "    \n",
    "    def fit(self, num_epochs):\n",
    "        \"\"\"Train the model for multiple epochs\"\"\"\n",
    "        print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        if Config.USE_MULTI_GPU:\n",
    "            print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc, val_f1, top5_acc = self.validate()\n",
    "            \n",
    "            # Update learning rate\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Save history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['val_f1'].append(val_f1)\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            print(f\"\\nEpoch Summary:\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "            print(f\"  Val F1: {val_f1:.4f}, Top-5 Acc: {top5_acc:.2f}%\")\n",
    "            print(f\"  Time: {epoch_time:.2f}s\")\n",
    "            print(f\"  LR: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > self.best_val_acc:\n",
    "                self.best_val_acc = val_acc\n",
    "                self.best_val_f1 = val_f1\n",
    "                self.epochs_without_improvement = 0\n",
    "                \n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_acc': val_acc,\n",
    "                    'val_f1': val_f1,\n",
    "                    'top5_acc': top5_acc\n",
    "                }, Config.CHECKPOINT_PATH)\n",
    "                print(f\"  âœ“ Best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
    "            else:\n",
    "                self.epochs_without_improvement += 1\n",
    "                print(f\"  No improvement for {self.epochs_without_improvement} epoch(s)\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.epochs_without_improvement >= Config.EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training completed!\")\n",
    "        print(f\"Best Validation Accuracy: {self.best_val_acc:.2f}%\")\n",
    "        print(f\"Best Validation F1-Score: {self.best_val_f1:.4f}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4cce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Comprehensive evaluation on test set\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='Testing'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            \n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_probs_concat = np.vstack(all_probs)\n",
    "    \n",
    "    top1_acc = 100. * accuracy_score(all_labels, all_preds)\n",
    "    top5_acc = 100. * top_k_accuracy_score(\n",
    "        all_labels, all_probs_concat, k=5, labels=range(Config.NUM_CLASSES)\n",
    "    )\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEST RESULTS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Top-1 Accuracy: {top1_acc:.2f}%\")\n",
    "    print(f\"Top-5 Accuracy: {top5_acc:.2f}%\")\n",
    "    print(f\"Macro F1-Score: {macro_f1:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "        'probabilities': all_probs_concat,\n",
    "        'top1_accuracy': top1_acc,\n",
    "        'top5_accuracy': top5_acc,\n",
    "        'macro_f1': macro_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886df837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "    axes[1].plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1-Score\n",
    "    axes[2].plot(history['val_f1'], label='Val F1', marker='s', color='green')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('F1-Score')\n",
    "    axes[2].set_title('Validation F1-Score')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.SAVE_DIR, 'training_history.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(labels, predictions, class_names):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    \n",
    "    plt.figure(figsize=(20, 18))\n",
    "    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.title('Confusion Matrix - Swin Transformer V1 Tiny', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=90, fontsize=8)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.SAVE_DIR, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "def analyze_predictions(results, class_names):\n",
    "    \"\"\"Analyze prediction results in detail\"\"\"\n",
    "    # Per-class accuracy\n",
    "    cm = confusion_matrix(results['labels'], results['predictions'])\n",
    "    \n",
    "    # Ensure we only use classes that exist in the test set\n",
    "    unique_labels = np.unique(results['labels'])\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    \n",
    "    # Handle cases where some classes might not be in test set\n",
    "    if len(per_class_acc) < len(class_names):\n",
    "        # Get only the class names that actually appear in test set\n",
    "        actual_class_names = [class_names[i] for i in unique_labels]\n",
    "    else:\n",
    "        actual_class_names = class_names\n",
    "    \n",
    "    # Ensure arrays have same length\n",
    "    min_len = min(len(actual_class_names), len(per_class_acc))\n",
    "    actual_class_names = actual_class_names[:min_len]\n",
    "    per_class_acc = per_class_acc[:min_len]\n",
    "    samples_per_class = cm.sum(axis=1)[:min_len]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    class_analysis = pd.DataFrame({\n",
    "        'Class': actual_class_names,\n",
    "        'Accuracy': per_class_acc * 100,\n",
    "        'Samples': samples_per_class\n",
    "    })\n",
    "    class_analysis = class_analysis.sort_values('Accuracy')\n",
    "    \n",
    "    print(\"\\nPer-Class Performance:\")\n",
    "    print(\"=\"*60)\n",
    "    print(class_analysis.to_string(index=False))\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Top 5 best and worst\n",
    "    print(\"\\nðŸ“Š TOP 5 BEST PERFORMING CLASSES:\")\n",
    "    print(class_analysis.tail(5).to_string(index=False))\n",
    "    \n",
    "    print(\"\\nðŸ“‰ TOP 5 WORST PERFORMING CLASSES:\")\n",
    "    print(class_analysis.head(5).to_string(index=False))\n",
    "    \n",
    "    # Save to CSV\n",
    "    class_analysis.to_csv(os.path.join(Config.SAVE_DIR, 'per_class_analysis.csv'), index=False)\n",
    "    \n",
    "    return class_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008bedb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"Swin Transformer V1 Tiny - Coffee Bean Classification\")\n",
    "    print(\"54 Indonesian Coffee Varieties\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check GPU availability\n",
    "    print(f\"\\nGPU Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Loading datasets...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    train_dataset = CoffeeDataset(Config.TRAIN_DIR, transform=get_transforms('train'))\n",
    "    val_dataset = CoffeeDataset(Config.VAL_DIR, transform=get_transforms('val'))\n",
    "    test_dataset = CoffeeDataset(Config.TEST_DIR, transform=get_transforms('test'))\n",
    "    \n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Creating model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model = create_model(num_classes=Config.NUM_CLASSES, pretrained=True)\n",
    "    \n",
    "    # Multi-GPU support\n",
    "    if Config.USE_MULTI_GPU:\n",
    "        print(f\"\\nWrapping model with DataParallel for {torch.cuda.device_count()} GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    model = model.to(Config.DEVICE)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=Config.LEARNING_RATE,\n",
    "        weight_decay=Config.WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=Config.NUM_EPOCHS,\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=Config.DEVICE\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = trainer.fit(Config.NUM_EPOCHS)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Load best model for testing\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Loading best model for testing...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    checkpoint = torch.load(Config.CHECKPOINT_PATH, weights_only=False)\n",
    "    # Load state dict - handle DataParallel wrapper\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    results = evaluate_model(model, test_loader, Config.DEVICE)\n",
    "    \n",
    "    # Visualizations\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    plot_confusion_matrix(results['labels'], results['predictions'], train_dataset.classes)\n",
    "    analyze_predictions(results, train_dataset.classes)\n",
    "    \n",
    "    # Save final results\n",
    "    final_results = {\n",
    "        'model': Config.MODEL_NAME,\n",
    "        'num_classes': Config.NUM_CLASSES,\n",
    "        'top1_accuracy': results['top1_accuracy'],\n",
    "        'top5_accuracy': results['top5_accuracy'],\n",
    "        'macro_f1': results['macro_f1'],\n",
    "        'train_samples': len(train_dataset),\n",
    "        'val_samples': len(val_dataset),\n",
    "        'test_samples': len(test_dataset)\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    import json\n",
    "    with open(os.path.join(Config.SAVE_DIR, 'final_results.json'), 'w') as f:\n",
    "        json.dump(final_results, f, indent=4)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training and evaluation completed successfully!\")\n",
    "    print(f\"Results saved to: {Config.SAVE_DIR}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
